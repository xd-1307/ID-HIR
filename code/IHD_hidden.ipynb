{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\")\n",
    "target = VGG('VGG16')\n",
    "pthfile = r\"checkpoints/VGG16_ckpt.pth\"\n",
    "d = torch.load(pthfile)['net']\n",
    "d = OrderedDict([(k[7:],v) for (k,v) in d.items()])\n",
    "target.load_state_dict(d)\n",
    "target = target.to(device)\n",
    "dae = torch.load(\"checkpoints/model.pkl\").to(device)\n",
    "dae.target = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Dataset\n",
    "attackPath = [\"../adv_data/cifar10/fgsm/0.015x_adv.npy\",\n",
    "              \"../adv_data/cifar10/fgsm/0.03x_adv.npy\",\n",
    "              \"../adv_data/cifar10/fgsm/0.06x_adv.npy\",\n",
    "              \"../adv_data/cifar10/cw/0.015x_adv.npy\",\n",
    "              \"../adv_data/cifar10/cw/0.03x_adv.npy\",\n",
    "              \"../adv_data/cifar10/cw/0.06x_adv.npy\",\n",
    "              \"../adv_data/cifar10/bim/0.015x_adv.npy\",\n",
    "              \"../adv_data/cifar10/bim/0.03x_adv.npy\",\n",
    "              \"../adv_data/cifar10/bim/0.06x_adv.npy\"\n",
    "             ]\n",
    "cleanPath = \"data/cifar10_data.npy\"\n",
    "labelPath = \"data/cifar10_label.npy\"\n",
    "dataset = Dataset(cleanPath = cleanPath, attackPath = attackPath, labelPath = labelPath,net = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adv_data = dataset.attackTrain[8]\n",
    "# batch = 50\n",
    "# h = []\n",
    "# for i in range(len(adv_data)//batch):\n",
    "#     data = adv_data[i*batch:(i+1)*batch]\n",
    "#     data = torch.from_numpy(data).to(device)\n",
    "#     data = dae(data)\n",
    "#     data = target.features(data)\n",
    "#     data = data.detach().cpu().numpy().squeeze()\n",
    "#     h.append(data)\n",
    "# h = np.array(h)\n",
    "# h = np.reshape(h,(-1,512))\n",
    "# np.save(\"data/hidden_cifar10_vgg16_bim16.npy\",h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = [\"data/hidden_cifar10_vgg16_fgsm4.npy\",\n",
    "              \"data/hidden_cifar10_vgg16_fgsm8.npy\",\n",
    "              \"data/hidden_cifar10_vgg16_fgsm16.npy\",\n",
    "              \"data/hidden_cifar10_vgg16_cw4.npy\",\n",
    "              \"data/hidden_cifar10_vgg16_cw8.npy\",\n",
    "              \"data/hidden_cifar10_vgg16_cw16.npy\",\n",
    "              \"data/hidden_cifar10_vgg16_bim4.npy\",\n",
    "              \"data/hidden_cifar10_vgg16_bim8.npy\",\n",
    "              \"data/hidden_cifar10_vgg16_bim16.npy\",\n",
    "             ]\n",
    "hidden_ = np.load(\"data/cifar10_vgg16_hidden.npy\")\n",
    "attack_data = [np.load(i) for i in attacks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_dataset():\n",
    "    for _ in range(1000):\n",
    "        data = []\n",
    "        label = []\n",
    "        for i in range(50):\n",
    "            s = random.randint(0,49999)\n",
    "            a = random.randint(0,8)\n",
    "            \n",
    "            x1 = attack_data[a][s]\n",
    "            x2 = hidden_[s]\n",
    "            lamda = random.uniform(0,1)\n",
    "            x = lamda*x1 + (1-lamda)*x2\n",
    "            \n",
    "            data.append(x1)\n",
    "            data.append(x2)\n",
    "            data.append(x)\n",
    "            label.append(dataset.labelTrain[s])\n",
    "            label.append(dataset.labelTrain[s])\n",
    "            label.append(dataset.labelTrain[s])\n",
    "            \n",
    "        yield np.array(data),np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 1.133\n",
      "epoch: 1  loss: 1.136\n",
      "epoch: 1  loss: 1.123\n",
      "epoch: 1  loss: 1.104\n",
      "epoch: 1  loss: 1.136\n",
      "evaluating\n",
      "Accuracy 90.5999984741211\n",
      "epoch: 2  loss: 1.090\n",
      "epoch: 2  loss: 1.148\n",
      "epoch: 2  loss: 1.126\n",
      "epoch: 2  loss: 1.126\n",
      "epoch: 2  loss: 1.117\n",
      "evaluating\n",
      "Accuracy 90.8699951171875\n",
      "epoch: 3  loss: 1.110\n",
      "epoch: 3  loss: 1.091\n",
      "epoch: 3  loss: 1.099\n",
      "epoch: 3  loss: 1.111\n",
      "epoch: 3  loss: 1.110\n",
      "evaluating\n",
      "Accuracy 90.75999450683594\n",
      "epoch: 4  loss: 1.136\n",
      "epoch: 4  loss: 1.141\n",
      "epoch: 4  loss: 1.087\n",
      "epoch: 4  loss: 1.092\n",
      "epoch: 4  loss: 1.083\n",
      "evaluating\n",
      "Accuracy 90.72000122070312\n",
      "epoch: 5  loss: 1.081\n",
      "epoch: 5  loss: 1.112\n",
      "epoch: 5  loss: 1.140\n",
      "epoch: 5  loss: 1.062\n",
      "epoch: 5  loss: 1.132\n",
      "evaluating\n",
      "Accuracy 90.81999969482422\n",
      "epoch: 6  loss: 1.108\n",
      "epoch: 6  loss: 1.082\n",
      "epoch: 6  loss: 1.137\n",
      "epoch: 6  loss: 1.109\n",
      "epoch: 6  loss: 1.097\n",
      "evaluating\n",
      "Accuracy 90.69999694824219\n",
      "epoch: 7  loss: 1.099\n",
      "epoch: 7  loss: 1.110\n",
      "epoch: 7  loss: 1.060\n",
      "epoch: 7  loss: 1.096\n",
      "epoch: 7  loss: 1.097\n",
      "evaluating\n",
      "Accuracy 90.88999938964844\n",
      "epoch: 8  loss: 1.101\n",
      "epoch: 8  loss: 1.083\n",
      "epoch: 8  loss: 1.129\n",
      "epoch: 8  loss: 1.122\n",
      "epoch: 8  loss: 1.059\n",
      "evaluating\n",
      "Accuracy 90.83999633789062\n",
      "epoch: 9  loss: 1.095\n",
      "epoch: 9  loss: 1.090\n",
      "epoch: 9  loss: 1.154\n",
      "epoch: 9  loss: 1.114\n",
      "epoch: 9  loss: 1.102\n",
      "evaluating\n",
      "Accuracy 90.65999603271484\n",
      "epoch: 10  loss: 1.075\n",
      "epoch: 10  loss: 1.131\n",
      "epoch: 10  loss: 1.103\n",
      "epoch: 10  loss: 1.092\n",
      "epoch: 10  loss: 1.086\n",
      "evaluating\n",
      "Accuracy 90.62999725341797\n",
      "epoch: 11  loss: 1.106\n",
      "epoch: 11  loss: 1.068\n",
      "epoch: 11  loss: 1.094\n",
      "epoch: 11  loss: 1.101\n",
      "epoch: 11  loss: 1.091\n",
      "evaluating\n",
      "Accuracy 90.72000122070312\n",
      "epoch: 12  loss: 1.071\n",
      "epoch: 12  loss: 1.082\n",
      "epoch: 12  loss: 1.082\n",
      "epoch: 12  loss: 1.082\n",
      "epoch: 12  loss: 1.066\n",
      "evaluating\n",
      "Accuracy 90.64999389648438\n",
      "epoch: 13  loss: 1.080\n",
      "epoch: 13  loss: 1.115\n",
      "epoch: 13  loss: 1.066\n",
      "epoch: 13  loss: 1.090\n",
      "epoch: 13  loss: 1.041\n",
      "evaluating\n",
      "Accuracy 90.70999908447266\n",
      "epoch: 14  loss: 1.076\n",
      "epoch: 14  loss: 1.087\n",
      "epoch: 14  loss: 1.075\n",
      "epoch: 14  loss: 1.085\n",
      "epoch: 14  loss: 1.064\n",
      "evaluating\n",
      "Accuracy 90.7699966430664\n",
      "epoch: 15  loss: 1.142\n",
      "epoch: 15  loss: 1.069\n",
      "epoch: 15  loss: 1.102\n",
      "epoch: 15  loss: 1.064\n",
      "epoch: 15  loss: 1.107\n",
      "evaluating\n",
      "Accuracy 90.50999450683594\n",
      "epoch: 16  loss: 1.091\n",
      "epoch: 16  loss: 1.072\n",
      "epoch: 16  loss: 1.095\n",
      "epoch: 16  loss: 1.108\n",
      "epoch: 16  loss: 1.122\n",
      "evaluating\n",
      "Accuracy 90.83999633789062\n",
      "epoch: 17  loss: 1.069\n",
      "epoch: 17  loss: 1.065\n",
      "epoch: 17  loss: 1.075\n",
      "epoch: 17  loss: 1.079\n",
      "epoch: 17  loss: 1.106\n",
      "evaluating\n",
      "Accuracy 90.62999725341797\n",
      "epoch: 18  loss: 1.106\n",
      "epoch: 18  loss: 1.035\n",
      "epoch: 18  loss: 1.112\n",
      "epoch: 18  loss: 1.101\n",
      "epoch: 18  loss: 1.083\n",
      "evaluating\n",
      "Accuracy 90.77999877929688\n",
      "epoch: 19  loss: 1.111\n",
      "epoch: 19  loss: 1.054\n",
      "epoch: 19  loss: 1.077\n",
      "epoch: 19  loss: 1.087\n",
      "epoch: 19  loss: 1.073\n",
      "evaluating\n",
      "Accuracy 90.52999877929688\n",
      "epoch: 20  loss: 1.085\n",
      "epoch: 20  loss: 1.102\n",
      "epoch: 20  loss: 1.055\n",
      "epoch: 20  loss: 1.080\n",
      "epoch: 20  loss: 1.061\n",
      "evaluating\n",
      "Accuracy 90.82999420166016\n",
      "epoch: 21  loss: 1.039\n",
      "epoch: 21  loss: 1.034\n",
      "epoch: 21  loss: 1.067\n",
      "epoch: 21  loss: 1.065\n",
      "epoch: 21  loss: 1.086\n",
      "evaluating\n",
      "Accuracy 90.62999725341797\n",
      "epoch: 22  loss: 1.078\n",
      "epoch: 22  loss: 1.053\n",
      "epoch: 22  loss: 1.033\n",
      "epoch: 22  loss: 1.104\n",
      "epoch: 22  loss: 1.102\n",
      "evaluating\n",
      "Accuracy 90.65999603271484\n",
      "epoch: 23  loss: 1.056\n",
      "epoch: 23  loss: 1.053\n",
      "epoch: 23  loss: 1.070\n",
      "epoch: 23  loss: 1.075\n",
      "epoch: 23  loss: 1.064\n",
      "evaluating\n",
      "Accuracy 90.66999816894531\n",
      "epoch: 24  loss: 1.053\n",
      "epoch: 24  loss: 1.049\n",
      "epoch: 24  loss: 1.028\n",
      "epoch: 24  loss: 1.053\n",
      "epoch: 24  loss: 1.104\n",
      "evaluating\n",
      "Accuracy 90.68000030517578\n",
      "epoch: 25  loss: 1.056\n",
      "epoch: 25  loss: 1.064\n",
      "epoch: 25  loss: 1.029\n",
      "epoch: 25  loss: 1.058\n",
      "epoch: 25  loss: 1.072\n",
      "evaluating\n",
      "Accuracy 90.79000091552734\n",
      "epoch: 26  loss: 1.058\n",
      "epoch: 26  loss: 1.114\n",
      "epoch: 26  loss: 1.060\n",
      "epoch: 26  loss: 1.045\n",
      "epoch: 26  loss: 1.041\n",
      "evaluating\n",
      "Accuracy 90.63999938964844\n",
      "epoch: 27  loss: 1.039\n",
      "epoch: 27  loss: 1.021\n",
      "epoch: 27  loss: 1.045\n",
      "epoch: 27  loss: 1.043\n",
      "epoch: 27  loss: 1.040\n",
      "evaluating\n",
      "Accuracy 90.7699966430664\n",
      "epoch: 28  loss: 1.034\n",
      "epoch: 28  loss: 1.034\n",
      "epoch: 28  loss: 1.081\n",
      "epoch: 28  loss: 1.077\n",
      "epoch: 28  loss: 1.073\n",
      "evaluating\n",
      "Accuracy 90.65999603271484\n",
      "epoch: 29  loss: 1.061\n",
      "epoch: 29  loss: 1.077\n",
      "epoch: 29  loss: 1.061\n",
      "epoch: 29  loss: 1.051\n",
      "epoch: 29  loss: 1.072\n",
      "evaluating\n",
      "Accuracy 90.5999984741211\n",
      "epoch: 30  loss: 1.024\n",
      "epoch: 30  loss: 1.050\n",
      "epoch: 30  loss: 1.052\n",
      "epoch: 30  loss: 1.024\n",
      "epoch: 30  loss: 1.048\n",
      "evaluating\n",
      "Accuracy 90.64999389648438\n",
      "epoch: 31  loss: 1.042\n",
      "epoch: 31  loss: 1.016\n",
      "epoch: 31  loss: 1.066\n",
      "epoch: 31  loss: 1.073\n",
      "epoch: 31  loss: 1.028\n",
      "evaluating\n",
      "Accuracy 90.72999572753906\n",
      "epoch: 32  loss: 1.039\n",
      "epoch: 32  loss: 1.071\n",
      "epoch: 32  loss: 1.045\n",
      "epoch: 32  loss: 1.069\n",
      "epoch: 32  loss: 1.049\n",
      "evaluating\n",
      "Accuracy 90.81999969482422\n",
      "epoch: 33  loss: 1.067\n",
      "epoch: 33  loss: 1.042\n",
      "epoch: 33  loss: 1.027\n",
      "epoch: 33  loss: 1.018\n",
      "epoch: 33  loss: 1.012\n",
      "evaluating\n",
      "Accuracy 90.6199951171875\n",
      "epoch: 34  loss: 1.035\n",
      "epoch: 34  loss: 1.064\n",
      "epoch: 34  loss: 1.051\n",
      "epoch: 34  loss: 1.013\n",
      "epoch: 34  loss: 1.012\n",
      "evaluating\n",
      "Accuracy 90.68000030517578\n",
      "epoch: 35  loss: 1.011\n",
      "epoch: 35  loss: 0.997\n",
      "epoch: 35  loss: 1.062\n",
      "epoch: 35  loss: 1.063\n",
      "epoch: 35  loss: 1.061\n",
      "evaluating\n",
      "Accuracy 90.68000030517578\n",
      "epoch: 36  loss: 1.080\n"
     ]
    }
   ],
   "source": [
    "from models import HiddenDenoiser\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "# hidden_denoiser = HiddenDenoiser().to(device)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, hidden_denoiser.parameters()))\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    dataGen = hidden_dataset()\n",
    "    for i, data in enumerate(dataGen, 1):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = torch.from_numpy(inputs).to(device),torch.from_numpy(labels).to(device)\n",
    "        inputs, labels = Variable(inputs,requires_grad=True), Variable(labels)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward\n",
    "        outputs = hidden_denoiser(inputs)\n",
    "        outputs = target.classifier(outputs)\n",
    "        # loss\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 0:  # print every 2000 mini-batches\n",
    "            print('epoch: %d  loss: %.3f' %\n",
    "                  (epoch + 1 , running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "    print(\"evaluating\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    testGen = dataset.testGen(8,clean = 1)\n",
    "    for data in testGen:\n",
    "        inputs, labels = data\n",
    "        inputs,labels = torch.from_numpy(inputs),torch.from_numpy(labels)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = dae(inputs)\n",
    "        outputs = target.features(outputs).squeeze()\n",
    "        outputs = hidden_denoiser(outputs)\n",
    "        outputs = target.classifier(outputs)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    print('Accuracy {}'.format(100. * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 90.91999816894531\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "testGen = dataset.testGen(8,clean = 1)\n",
    "for data in testGen:\n",
    "    inputs, labels = data\n",
    "    inputs,labels = torch.from_numpy(inputs),torch.from_numpy(labels)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    outputs = dae(inputs)\n",
    "    outputs = target.features(outputs).squeeze()\n",
    "#     outputs = hidden_denoiser(outputs)\n",
    "    outputs = target.classifier(outputs)\n",
    "    \n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "print('Accuracy {}'.format(100. * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
